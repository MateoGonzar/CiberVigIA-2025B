{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c483d56-daad-445f-8e55-6dd2a354c66e",
   "metadata": {},
   "source": [
    "# üìä Fase 2 ‚Äì Dataset y Preprocesamiento\n",
    "\n",
    "## üéØ Objetivo\n",
    "Preparar un conjunto de datos limpio y estructurado para entrenar un modelo de Machine Learning enfocado en la detecci√≥n de intrusiones en redes, como parte del proyecto modular **CiberVigIA 2025B**.\n",
    "\n",
    "## üìÅ Datasets sugeridos\n",
    "- **NSL-KDD**: Dataset cl√°sico para sistemas de detecci√≥n de intrusos (IDS), √∫til para benchmarking.\n",
    "- **CICIDS 2017**: Dataset m√°s realista y contempor√°neo, con tr√°fico simulado que incluye ataques modernos.\n",
    "\n",
    "## üß™ Pasos t√©cnicos\n",
    "\n",
    "1. **Descarga del dataset**  \n",
    "   - Formato: `.csv`  \n",
    "   - Fuente: Repositorio oficial o mirror institucional\n",
    "\n",
    "2. **Limpieza de datos**  \n",
    "   - Eliminaci√≥n de columnas irrelevantes (por ejemplo: `timestamp`, `flow_id`, etc.)  \n",
    "   - Revisi√≥n de valores nulos o inconsistentes  \n",
    "   - Normalizaci√≥n de nombres de columnas\n",
    "\n",
    "3. **Codificaci√≥n de variables categ√≥ricas**  \n",
    "   - Ejemplo: `protocol_type` ‚Üí TCP / UDP / ICMP  \n",
    "   - T√©cnicas: One-Hot Encoding o Label Encoding seg√∫n el modelo a utilizar\n",
    "\n",
    "4. **Verificaci√≥n de balance de clases**  \n",
    "   - Identificaci√≥n de clases mayoritarias/minoritarias  \n",
    "   - Posible aplicaci√≥n de t√©cnicas como SMOTE para balancear\n",
    "\n",
    "5. **Exportaci√≥n del dataset preprocesado**  \n",
    "   - Formato final: `CSV` limpio y listo para entrenamiento  \n",
    "   - Ubicaci√≥n: `data/processed/` dentro del repositorio\n",
    "\n",
    "## üì¶ Entregable\n",
    "Un archivo `.csv` con los datos preprocesados, documentado y listo para ser utilizado en la Fase 3 (Entrenamiento del modelo ML).\n",
    "\n",
    "---\n",
    "\n",
    "> üß† *Este notebook forma parte del pipeline modular del proyecto CiberVigIA, alineado con objetivos de sostenibilidad, seguridad inform√°tica y an√°lisis de tr√°fico de red mediante inteligencia artificial.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54120a46-612e-4f1a-a6fc-92f0d39d6538",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134e1bc1-0198-4d44-b290-7f373386e9ea",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Funci√≥n de ayuda para obtener los nombres de las columnas en el dataset de KDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ca1a102-5b40-49c3-ab4f-c12f8b391539",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_column_names(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    column_names = []\n",
    "    attack_types_raw = lines[0].strip()  # L√≠nea 0: tipos de ataques\n",
    "    attack_types = [attack.strip() for attack in attack_types_raw.split(',')]  # Parsear en lista\n",
    "    for line in lines:  # Empieza desde l√≠nea 2\n",
    "        if ':' in line:  # Ignora l√≠neas sin \":\"\n",
    "            name = line.split(':')[0].strip()  # Toma antes de \":\", quita espacios\n",
    "            column_names.append(name)\n",
    "    column_names += ['label']\n",
    "    return attack_types, column_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98cb8d65-194d-402b-a96b-2a9046c68c17",
   "metadata": {},
   "source": [
    "### Paso 1. Carga de Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6e3319a-934c-414c-9b73-44e1cfa6ad02",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../data/raw/KDD CUP 99/corrected.gz', header=None)\n",
    "column_names = get_column_names('../../data/raw/KDD CUP 99/kddcup.names')\n",
    "attack_types, df.columns = column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3fb159be-0279-411d-b018-6b6a5330d569",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = pd.read_csv('../../data/raw/IDS 2017/Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv')\n",
    "data2 = pd.read_csv('../../data/raw/IDS 2017/Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv')\n",
    "data3 = pd.read_csv('../../data/raw/IDS 2017/Friday-WorkingHours-Morning.pcap_ISCX.csv')\n",
    "data4 = pd.read_csv('../../data/raw/IDS 2017/Monday-WorkingHours.pcap_ISCX.csv')\n",
    "data5 = pd.read_csv('../../data/raw/IDS 2017/Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv')\n",
    "data6 = pd.read_csv('../../data/raw/IDS 2017/Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv')\n",
    "data7 = pd.read_csv('../../data/raw/IDS 2017/Tuesday-WorkingHours.pcap_ISCX.csv')\n",
    "data8 = pd.read_csv('../../data/raw/IDS 2017/Wednesday-workingHours.pcap_ISCX.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8ae329-7aaf-4626-a381-93c10373602f",
   "metadata": {},
   "source": [
    "### Paso 2. Agrupar Dataset de CIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83a990b8-d5c4-4ecb-ae1b-c47e2cf649a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = [data1, data2, data3, data4, data5, data6, data7, data8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6e89d2b-fb83-45bb-a79d-370b19c41fda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data dimensions: \n",
      "Data1 -> 225745 rows, 79 columns\n",
      "Data2 -> 286467 rows, 79 columns\n",
      "Data3 -> 191033 rows, 79 columns\n",
      "Data4 -> 529918 rows, 79 columns\n",
      "Data5 -> 288602 rows, 79 columns\n",
      "Data6 -> 170366 rows, 79 columns\n",
      "Data7 -> 445909 rows, 79 columns\n",
      "Data8 -> 692703 rows, 79 columns\n"
     ]
    }
   ],
   "source": [
    "print('Data dimensions: ')\n",
    "for i, data in enumerate(data_list, start = 1):\n",
    "  rows, cols = data.shape\n",
    "  print(f'Data{i} -> {rows} rows, {cols} columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07a593ed-1e3d-4a97-8213-0b4f623329e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat(data_list) # Dataset CIC 2017 concatenado\n",
    "rows, cols = data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d3d42134-a482-4ba8-b261-1e6a11ab2086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New dimension for CIC Dataset:\n",
      "Number of rows: 2830743\n",
      "Number of columns: 79\n",
      "Total cells: 223628697\n"
     ]
    }
   ],
   "source": [
    "print('New dimension for CIC Dataset:')\n",
    "print(f'Number of rows: {rows}')\n",
    "print(f'Number of columns: {cols}')\n",
    "print(f'Total cells: {rows * cols}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ef13725-ffc8-4d48-9788-600a617fa86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in data_list: del d # Limpieza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1feaa7c0-f9a9-4005-953f-9c5c49f1734e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data KDD -> 311029 rows, 42 columns\n"
     ]
    }
   ],
   "source": [
    "rows, cols = df.shape\n",
    "print(f'Data KDD -> {rows} rows, {cols} columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067fcc61-3374-4fd5-8eba-7935ce8cb467",
   "metadata": {},
   "source": [
    "### Paso 3: Mostrar los nombres de las columnas y exploraci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "84d9888d-d00b-45b9-a441-a304712f79ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data KDD -> Index(['duration', 'protocol_type', 'service', 'flag', 'src_bytes',\n",
      "       'dst_bytes', 'land', 'wrong_fragment', 'urgent', 'hot',\n",
      "       'num_failed_logins', 'logged_in', 'num_compromised', 'root_shell',\n",
      "       'su_attempted', 'num_root', 'num_file_creations', 'num_shells',\n",
      "       'num_access_files', 'num_outbound_cmds', 'is_host_login',\n",
      "       'is_guest_login', 'count', 'srv_count', 'serror_rate',\n",
      "       'srv_serror_rate', 'rerror_rate', 'srv_rerror_rate', 'same_srv_rate',\n",
      "       'diff_srv_rate', 'srv_diff_host_rate', 'dst_host_count',\n",
      "       'dst_host_srv_count', 'dst_host_same_srv_rate',\n",
      "       'dst_host_diff_srv_rate', 'dst_host_same_src_port_rate',\n",
      "       'dst_host_srv_diff_host_rate', 'dst_host_serror_rate',\n",
      "       'dst_host_srv_serror_rate', 'dst_host_rerror_rate',\n",
      "       'dst_host_srv_rerror_rate', 'label'],\n",
      "      dtype='object')\n",
      "Data CIC -> Index([' Destination Port', ' Flow Duration', ' Total Fwd Packets',\n",
      "       ' Total Backward Packets', 'Total Length of Fwd Packets',\n",
      "       ' Total Length of Bwd Packets', ' Fwd Packet Length Max',\n",
      "       ' Fwd Packet Length Min', ' Fwd Packet Length Mean',\n",
      "       ' Fwd Packet Length Std', 'Bwd Packet Length Max',\n",
      "       ' Bwd Packet Length Min', ' Bwd Packet Length Mean',\n",
      "       ' Bwd Packet Length Std', 'Flow Bytes/s', ' Flow Packets/s',\n",
      "       ' Flow IAT Mean', ' Flow IAT Std', ' Flow IAT Max', ' Flow IAT Min',\n",
      "       'Fwd IAT Total', ' Fwd IAT Mean', ' Fwd IAT Std', ' Fwd IAT Max',\n",
      "       ' Fwd IAT Min', 'Bwd IAT Total', ' Bwd IAT Mean', ' Bwd IAT Std',\n",
      "       ' Bwd IAT Max', ' Bwd IAT Min', 'Fwd PSH Flags', ' Bwd PSH Flags',\n",
      "       ' Fwd URG Flags', ' Bwd URG Flags', ' Fwd Header Length',\n",
      "       ' Bwd Header Length', 'Fwd Packets/s', ' Bwd Packets/s',\n",
      "       ' Min Packet Length', ' Max Packet Length', ' Packet Length Mean',\n",
      "       ' Packet Length Std', ' Packet Length Variance', 'FIN Flag Count',\n",
      "       ' SYN Flag Count', ' RST Flag Count', ' PSH Flag Count',\n",
      "       ' ACK Flag Count', ' URG Flag Count', ' CWE Flag Count',\n",
      "       ' ECE Flag Count', ' Down/Up Ratio', ' Average Packet Size',\n",
      "       ' Avg Fwd Segment Size', ' Avg Bwd Segment Size',\n",
      "       ' Fwd Header Length.1', 'Fwd Avg Bytes/Bulk', ' Fwd Avg Packets/Bulk',\n",
      "       ' Fwd Avg Bulk Rate', ' Bwd Avg Bytes/Bulk', ' Bwd Avg Packets/Bulk',\n",
      "       'Bwd Avg Bulk Rate', 'Subflow Fwd Packets', ' Subflow Fwd Bytes',\n",
      "       ' Subflow Bwd Packets', ' Subflow Bwd Bytes', 'Init_Win_bytes_forward',\n",
      "       ' Init_Win_bytes_backward', ' act_data_pkt_fwd',\n",
      "       ' min_seg_size_forward', 'Active Mean', ' Active Std', ' Active Max',\n",
      "       ' Active Min', 'Idle Mean', ' Idle Std', ' Idle Max', ' Idle Min',\n",
      "       ' Label'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(f'Data KDD -> {df.columns}')\n",
    "print(f'Data CIC -> {data.columns}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "96fe8c70-6db4-4f79-837c-39adf6bacf79",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns = data.columns.str.lstrip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3899fb21-5dc2-41fb-ac58-3fcbb0622abc",
   "metadata": {},
   "source": [
    "Limpiamos las columnas no necesarias de cada uno:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3448f169-cb3c-44a7-9b1a-3c15ec79dbcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No capturables quitadas. Nueva forma: (311029, 30)\n"
     ]
    }
   ],
   "source": [
    "non_capturable = [\n",
    "    'hot', # Host-Based\n",
    "    'num_failed_logins', # Requiere logs host.\n",
    "    'logged_in', # Host-Based\n",
    "    'num_compromised', # Correlaci√≥n externa necesatia \n",
    "    'root_shell', # No es fiable\n",
    "    'su_attempt', # Depende de payload\n",
    "    'num_root', # Requiere logs host.\n",
    "    'num_file_creations',  # Requiere host monitoring o inspecci√≥n profunda.\n",
    "    'num_shells', # No fiable en encriptado\n",
    "    'num_access_files', # Host-Based\n",
    "    'num_outbound_cmds', # Parsing profundo\n",
    "    'is_host_login', # No siempre capturable\n",
    "    'is_guest_login', # No siempre capturable\n",
    "]\n",
    "\n",
    "df = df.drop(columns=[col for col in non_capturable if col in df.columns])\n",
    "print(\"No capturables quitadas. Nueva forma:\", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "25ad3081-13b4-404a-bbc5-b9d458d00361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data KDD:\n",
      "Forma original: (311029, 30)\n",
      "Tipos: duration                         int64\n",
      "protocol_type                   object\n",
      "service                         object\n",
      "flag                            object\n",
      "src_bytes                        int64\n",
      "dst_bytes                        int64\n",
      "land                             int64\n",
      "wrong_fragment                   int64\n",
      "urgent                           int64\n",
      "su_attempted                     int64\n",
      "count                            int64\n",
      "srv_count                        int64\n",
      "serror_rate                    float64\n",
      "srv_serror_rate                float64\n",
      "rerror_rate                    float64\n",
      "srv_rerror_rate                float64\n",
      "same_srv_rate                  float64\n",
      "diff_srv_rate                  float64\n",
      "srv_diff_host_rate             float64\n",
      "dst_host_count                   int64\n",
      "dst_host_srv_count               int64\n",
      "dst_host_same_srv_rate         float64\n",
      "dst_host_diff_srv_rate         float64\n",
      "dst_host_same_src_port_rate    float64\n",
      "dst_host_srv_diff_host_rate    float64\n",
      "dst_host_serror_rate           float64\n",
      "dst_host_srv_serror_rate       float64\n",
      "dst_host_rerror_rate           float64\n",
      "dst_host_srv_rerror_rate       float64\n",
      "label                           object\n",
      "dtype: object\n",
      "Missing values: 0\n",
      "Duplicados: 233766\n",
      "Balance de labels: label\n",
      "smurf.              164091\n",
      "normal.              60593\n",
      "neptune.             58001\n",
      "snmpgetattack.        7741\n",
      "mailbomb.             5000\n",
      "guess_passwd.         4367\n",
      "snmpguess.            2406\n",
      "satan.                1633\n",
      "warezmaster.          1602\n",
      "back.                 1098\n",
      "mscan.                1053\n",
      "apache2.               794\n",
      "processtable.          759\n",
      "saint.                 736\n",
      "portsweep.             354\n",
      "ipsweep.               306\n",
      "httptunnel.            158\n",
      "pod.                    87\n",
      "nmap.                   84\n",
      "buffer_overflow.        22\n",
      "multihop.               18\n",
      "named.                  17\n",
      "sendmail.               17\n",
      "ps.                     16\n",
      "rootkit.                13\n",
      "xterm.                  13\n",
      "teardrop.               12\n",
      "xlock.                   9\n",
      "land.                    9\n",
      "xsnoop.                  4\n",
      "ftp_write.               3\n",
      "loadmodule.              2\n",
      "perl.                    2\n",
      "udpstorm.                2\n",
      "worm.                    2\n",
      "phf.                     2\n",
      "sqlattack.               2\n",
      "imap.                    1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print('Data KDD:')\n",
    "print(\"Forma original:\", df.shape)\n",
    "print(\"Tipos:\", df.dtypes)\n",
    "print(\"Missing values:\", df.isnull().sum().sum())\n",
    "print(\"Duplicados:\", df.duplicated().sum())\n",
    "label_counts = df['label'].value_counts() if 'label' in df else pd.Series()\n",
    "print(\"Balance de labels:\", label_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36866ac5-a04d-4fdd-8142-de96cc4598bd",
   "metadata": {},
   "source": [
    "Filtrar clases con pocas muestras (evita error SMOTE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "54960358-de45-4bf5-b36f-dd34e5e91b4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtrando clases con <2 muestras: Index(['imap.'], dtype='object', name='label')\n"
     ]
    }
   ],
   "source": [
    "low_sample_classes = label_counts[label_counts < 2].index\n",
    "if len(low_sample_classes) > 0:\n",
    "    print(f\"Filtrando clases con <2 muestras: {low_sample_classes}\")\n",
    "    df = df[~df['label'].isin(low_sample_classes)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4cd3481b-f7a9-44fb-8090-8bf4419c209a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data CIC:\n",
      "Forma original: (2830743, 79)\n",
      "Tipos: Destination Port                 int64\n",
      "Flow Duration                    int64\n",
      "Total Fwd Packets                int64\n",
      "Total Backward Packets           int64\n",
      "Total Length of Fwd Packets      int64\n",
      "                                ...   \n",
      "Idle Mean                      float64\n",
      "Idle Std                       float64\n",
      "Idle Max                         int64\n",
      "Idle Min                         int64\n",
      "Label                           object\n",
      "Length: 79, dtype: object\n",
      "Missing values: 1358\n",
      "Duplicados: 308381\n",
      "Balance de labels: Label\n",
      "BENIGN                        2273097\n",
      "DoS Hulk                       231073\n",
      "PortScan                       158930\n",
      "DDoS                           128027\n",
      "DoS GoldenEye                   10293\n",
      "FTP-Patator                      7938\n",
      "SSH-Patator                      5897\n",
      "DoS slowloris                    5796\n",
      "DoS Slowhttptest                 5499\n",
      "Bot                              1966\n",
      "Web Attack ÔøΩ Brute Force         1507\n",
      "Web Attack ÔøΩ XSS                  652\n",
      "Infiltration                       36\n",
      "Web Attack ÔøΩ Sql Injection         21\n",
      "Heartbleed                         11\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print('Data CIC:')\n",
    "print(\"Forma original:\", data.shape)\n",
    "print(\"Tipos:\", data.dtypes)\n",
    "print(\"Missing values:\", data.isnull().sum().sum())\n",
    "print(\"Duplicados:\", data.duplicated().sum())\n",
    "label_counts = data['Label'].value_counts() if 'Label' in data else pd.Series()\n",
    "print(\"Balance de labels:\", label_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a548701-fc89-46bd-8d9c-243af898813c",
   "metadata": {},
   "source": [
    "Filtrar clases con pocas muestras (evita error SMOTE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8cb44020-0e82-4a51-9b34-0156384d49e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "low_sample_classes = label_counts[label_counts < 2].index\n",
    "if len(low_sample_classes) > 0:\n",
    "    print(f\"Filtrando clases con <2 muestras: {low_sample_classes}\")\n",
    "    data = data[~data['Label'].isin(low_sample_classes)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa16a305-7c10-412b-9b41-aaa7b70d50ef",
   "metadata": {},
   "source": [
    "### Paso 4: Limpiamos los datos perdidos / duplicados / infinitos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "44546a8e-2c14-4c7b-b03b-175d0f941a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.replace([np.inf, -np.inf], np.nan).fillna(0).drop_duplicates()\n",
    "data = data.replace([np.inf, -np.inf], np.nan).fillna(0).drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d24ff16-d6f2-416e-bb0c-cdca0017aa76",
   "metadata": {},
   "source": [
    "### Paso 5: Agegamos encodings categoricos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f722bc06-fa9f-4e66-9a9b-7239fbf7831f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lekdd = LabelEncoder()\n",
    "for col in ['protocol_type', 'service', 'flag']:\n",
    "    df[col] = lekdd.fit_transform(df[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f644fd17-bccf-4ce9-8a4f-b5d0478cea2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lecic = LabelEncoder()\n",
    "for col in data.select_dtypes(include=['object']).columns:\n",
    "    if col != 'Label':\n",
    "        data[col] = lecic.fit_transform(data[col])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e70143-5a5a-4072-8fe0-bddd42ef4452",
   "metadata": {},
   "source": [
    "### Paso 6: Scaling numerico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0251b515-9525-40d8-ae73-9e3ca6e76bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_cols_kdd = df.select_dtypes(include=['int64', 'float64']).columns.drop('label', errors='ignore')\n",
    "scaler_kdd = StandardScaler()\n",
    "df[numerical_cols_kdd] = scaler_kdd.fit_transform(df[numerical_cols_kdd])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "920735a4-c63a-48ac-bb90-c3b94ff07cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_cols_cic = data.select_dtypes(include=['int64', 'float64']).columns.drop('Label', errors='ignore')\n",
    "scaler_cic = StandardScaler()\n",
    "data[numerical_cols_cic] = scaler_cic.fit_transform(data[numerical_cols_cic])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e742269b-0d9a-483d-b5c0-8373d28a5c06",
   "metadata": {},
   "source": [
    "### Paso 7: Balanceo (SMOTE) -> Se necesita en ambos datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "10be0194-6fb7-4a15-a681-d59c7f253c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smote_por_chunks(X, y, chunk_size=50000, random_state=42, k_neighbors=1):\n",
    "    \"\"\"\n",
    "    Aplica SMOTE por bloques. Si SMOTE falla en un chunk, usa RandomOverSampler como respaldo.\n",
    "    Retorna un DataFrame balanceado.\n",
    "    \"\"\"\n",
    "    X_res_total, y_res_total = [], []\n",
    "\n",
    "    # Identifica la clase minoritaria\n",
    "    clase_min = y.value_counts().idxmin()\n",
    "\n",
    "    # Separa las clases\n",
    "    X_min, y_min = X[y == clase_min], y[y == clase_min]\n",
    "    X_maj, y_maj = X[y != clase_min], y[y != clase_min]\n",
    "\n",
    "    # Itera sobre la clase mayoritaria en bloques\n",
    "    for i in range(0, len(X_maj), chunk_size):\n",
    "        X_chunk = pd.concat([X_min, X_maj.iloc[i:i + chunk_size]])\n",
    "        y_chunk = pd.concat([y_min, y_maj.iloc[i:i + chunk_size]])\n",
    "\n",
    "        try:\n",
    "            # Intenta aplicar SMOTE\n",
    "            smote = SMOTE(random_state=random_state, k_neighbors=k_neighbors)\n",
    "            X_res, y_res = smote.fit_resample(X_chunk, y_chunk)\n",
    "            print(f\"Chunk {i}: SMOTE aplicado correctamente.\")\n",
    "        except Exception as e:\n",
    "            # Si SMOTE falla, usa RandomOverSampler\n",
    "            print(f\"Chunk {i} fall√≥ con SMOTE: {e}. Usando RandomOverSampler.\")\n",
    "            ros = RandomOverSampler(random_state=random_state)\n",
    "            X_res, y_res = ros.fit_resample(X_chunk, y_chunk)\n",
    "\n",
    "        # Guarda los resultados del chunk\n",
    "        X_res_total.append(pd.DataFrame(X_res, columns=X.columns))\n",
    "        y_res_total.append(pd.Series(y_res))\n",
    "\n",
    "    # Une todos los chunks procesados\n",
    "    X_final = pd.concat(X_res_total, ignore_index=True)\n",
    "    y_final = pd.concat(y_res_total, ignore_index=True)\n",
    "\n",
    "    return pd.concat([X_final, pd.Series(y_final, name='Label')], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d7066f83-fa48-4697-a14c-a09e95d493ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 0 fall√≥ con SMOTE: Expected n_neighbors <= n_samples_fit, but n_neighbors = 2, n_samples_fit = 1, n_samples = 1. Usando RandomOverSampler.\n",
      "Chunk 50000 fall√≥ con SMOTE: Expected n_neighbors <= n_samples_fit, but n_neighbors = 2, n_samples_fit = 1, n_samples = 1. Usando RandomOverSampler.\n"
     ]
    }
   ],
   "source": [
    "X = df.drop('label', axis=1)\n",
    "y = df['label']\n",
    "df_balanced = smote_por_chunks(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fed2a163-da02-4e49-a706-4c0f6a840dc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 0: SMOTE aplicado correctamente.\n",
      "Chunk 50000: SMOTE aplicado correctamente.\n",
      "Chunk 100000: SMOTE aplicado correctamente.\n",
      "Chunk 150000: SMOTE aplicado correctamente.\n",
      "Chunk 200000: SMOTE aplicado correctamente.\n",
      "Chunk 250000: SMOTE aplicado correctamente.\n",
      "Chunk 300000: SMOTE aplicado correctamente.\n",
      "Chunk 350000: SMOTE aplicado correctamente.\n",
      "Chunk 400000: SMOTE aplicado correctamente.\n",
      "Chunk 450000: SMOTE aplicado correctamente.\n",
      "Chunk 500000: SMOTE aplicado correctamente.\n",
      "Chunk 550000: SMOTE aplicado correctamente.\n",
      "Chunk 600000: SMOTE aplicado correctamente.\n",
      "Chunk 650000: SMOTE aplicado correctamente.\n",
      "Chunk 700000: SMOTE aplicado correctamente.\n",
      "Chunk 750000: SMOTE aplicado correctamente.\n",
      "Chunk 800000: SMOTE aplicado correctamente.\n",
      "Chunk 850000: SMOTE aplicado correctamente.\n",
      "Chunk 900000: SMOTE aplicado correctamente.\n",
      "Chunk 950000: SMOTE aplicado correctamente.\n",
      "Chunk 1000000: SMOTE aplicado correctamente.\n",
      "Chunk 1050000: SMOTE aplicado correctamente.\n",
      "Chunk 1100000: SMOTE aplicado correctamente.\n",
      "Chunk 1150000: SMOTE aplicado correctamente.\n",
      "Chunk 1200000: SMOTE aplicado correctamente.\n",
      "Chunk 1250000: SMOTE aplicado correctamente.\n",
      "Chunk 1300000: SMOTE aplicado correctamente.\n",
      "Chunk 1350000: SMOTE aplicado correctamente.\n",
      "Chunk 1400000: SMOTE aplicado correctamente.\n",
      "Chunk 1450000: SMOTE aplicado correctamente.\n",
      "Chunk 1500000: SMOTE aplicado correctamente.\n",
      "Chunk 1550000: SMOTE aplicado correctamente.\n",
      "Chunk 1600000: SMOTE aplicado correctamente.\n",
      "Chunk 1650000: SMOTE aplicado correctamente.\n",
      "Chunk 1700000: SMOTE aplicado correctamente.\n",
      "Chunk 1750000: SMOTE aplicado correctamente.\n",
      "Chunk 1800000: SMOTE aplicado correctamente.\n",
      "Chunk 1850000: SMOTE aplicado correctamente.\n",
      "Chunk 1900000 fall√≥ con SMOTE: Expected n_neighbors <= n_samples_fit, but n_neighbors = 2, n_samples_fit = 1, n_samples = 1. Usando RandomOverSampler.\n",
      "Chunk 1950000: SMOTE aplicado correctamente.\n",
      "Chunk 2000000: SMOTE aplicado correctamente.\n",
      "Chunk 2050000: SMOTE aplicado correctamente.\n",
      "Chunk 2100000: SMOTE aplicado correctamente.\n",
      "Chunk 2150000: SMOTE aplicado correctamente.\n",
      "Chunk 2200000: SMOTE aplicado correctamente.\n",
      "Chunk 2250000: SMOTE aplicado correctamente.\n",
      "Chunk 2300000: SMOTE aplicado correctamente.\n",
      "Chunk 2350000: SMOTE aplicado correctamente.\n",
      "Chunk 2400000: SMOTE aplicado correctamente.\n",
      "Chunk 2450000: SMOTE aplicado correctamente.\n",
      "Chunk 2500000: SMOTE aplicado correctamente.\n"
     ]
    }
   ],
   "source": [
    "X = data.drop('Label', axis=1)\n",
    "y = data['Label']\n",
    "data_balanced = smote_por_chunks(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd809473-8c13-4d47-86a0-a25579cb713e",
   "metadata": {},
   "source": [
    "### Paso 8: Guardar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4bf166e8-2a2a-4505-a7f3-461305509e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def guardar_por_chunks(df, ruta_csv, chunk_size=100000):\n",
    "    # Si el archivo ya existe, lo eliminamos para empezar limpio\n",
    "    import os\n",
    "    if os.path.exists(ruta_csv):\n",
    "        os.remove(ruta_csv)\n",
    "\n",
    "    # Guardar por bloques\n",
    "    for i in range(0, len(df), chunk_size):\n",
    "        chunk = df.iloc[i:i+chunk_size]\n",
    "        # Solo escribe el encabezado en el primer chunk\n",
    "        chunk.to_csv(ruta_csv, mode='a', index=False, header=(i==0))\n",
    "        print(f\"Chunk {i} guardado: filas {i} a {i+len(chunk)-1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cf78286f-3bc0-4056-88fc-ebdb3f7f122d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 0 guardado: filas 0 a 99999\n",
      "Chunk 100000 guardado: filas 100000 a 199999\n",
      "Chunk 200000 guardado: filas 200000 a 299999\n",
      "Chunk 300000 guardado: filas 300000 a 399999\n",
      "Chunk 400000 guardado: filas 400000 a 499999\n",
      "Chunk 500000 guardado: filas 500000 a 599999\n",
      "Chunk 600000 guardado: filas 600000 a 699999\n",
      "Chunk 700000 guardado: filas 700000 a 799999\n",
      "Chunk 800000 guardado: filas 800000 a 899999\n",
      "Chunk 900000 guardado: filas 900000 a 999999\n",
      "Chunk 1000000 guardado: filas 1000000 a 1099999\n",
      "Chunk 1100000 guardado: filas 1100000 a 1199999\n",
      "Chunk 1200000 guardado: filas 1200000 a 1299999\n",
      "Chunk 1300000 guardado: filas 1300000 a 1399999\n",
      "Chunk 1400000 guardado: filas 1400000 a 1450115\n",
      "NSL-KDD limpiado guardado por chunks.\n"
     ]
    }
   ],
   "source": [
    "guardar_por_chunks(df_balanced, '../../data/processed/nsl_kdd_clean.csv')\n",
    "print(\"NSL-KDD limpiado guardado por chunks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f36aeb76-a5ef-49af-86c5-ba9e0bfc3b69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 0 guardado: filas 0 a 99999\n",
      "Chunk 100000 guardado: filas 100000 a 199999\n",
      "Chunk 200000 guardado: filas 200000 a 299999\n",
      "Chunk 300000 guardado: filas 300000 a 399999\n",
      "Chunk 400000 guardado: filas 400000 a 499999\n",
      "Chunk 500000 guardado: filas 500000 a 599999\n",
      "Chunk 600000 guardado: filas 600000 a 699999\n",
      "Chunk 700000 guardado: filas 700000 a 799999\n",
      "Chunk 800000 guardado: filas 800000 a 899999\n",
      "Chunk 900000 guardado: filas 900000 a 999999\n",
      "Chunk 1000000 guardado: filas 1000000 a 1099999\n",
      "Chunk 1100000 guardado: filas 1100000 a 1199999\n",
      "Chunk 1200000 guardado: filas 1200000 a 1299999\n",
      "Chunk 1300000 guardado: filas 1300000 a 1399999\n",
      "Chunk 1400000 guardado: filas 1400000 a 1499999\n",
      "Chunk 1500000 guardado: filas 1500000 a 1599999\n",
      "Chunk 1600000 guardado: filas 1600000 a 1699999\n",
      "Chunk 1700000 guardado: filas 1700000 a 1799999\n",
      "Chunk 1800000 guardado: filas 1800000 a 1899999\n",
      "Chunk 1900000 guardado: filas 1900000 a 1999999\n",
      "Chunk 2000000 guardado: filas 2000000 a 2099999\n",
      "Chunk 2100000 guardado: filas 2100000 a 2199999\n",
      "Chunk 2200000 guardado: filas 2200000 a 2299999\n",
      "Chunk 2300000 guardado: filas 2300000 a 2399999\n",
      "Chunk 2400000 guardado: filas 2400000 a 2499999\n",
      "Chunk 2500000 guardado: filas 2500000 a 2599999\n",
      "Chunk 2600000 guardado: filas 2600000 a 2699999\n",
      "Chunk 2700000 guardado: filas 2700000 a 2799999\n",
      "Chunk 2800000 guardado: filas 2800000 a 2899999\n",
      "Chunk 2900000 guardado: filas 2900000 a 2999999\n",
      "Chunk 3000000 guardado: filas 3000000 a 3099999\n",
      "Chunk 3100000 guardado: filas 3100000 a 3199999\n",
      "Chunk 3200000 guardado: filas 3200000 a 3299999\n",
      "Chunk 3300000 guardado: filas 3300000 a 3399999\n",
      "Chunk 3400000 guardado: filas 3400000 a 3499999\n",
      "Chunk 3500000 guardado: filas 3500000 a 3599999\n",
      "Chunk 3600000 guardado: filas 3600000 a 3699999\n",
      "Chunk 3700000 guardado: filas 3700000 a 3799999\n",
      "Chunk 3800000 guardado: filas 3800000 a 3899999\n",
      "Chunk 3900000 guardado: filas 3900000 a 3999999\n",
      "Chunk 4000000 guardado: filas 4000000 a 4099999\n",
      "Chunk 4100000 guardado: filas 4100000 a 4199999\n",
      "Chunk 4200000 guardado: filas 4200000 a 4299999\n",
      "Chunk 4300000 guardado: filas 4300000 a 4399999\n",
      "Chunk 4400000 guardado: filas 4400000 a 4499999\n",
      "Chunk 4500000 guardado: filas 4500000 a 4599999\n",
      "Chunk 4600000 guardado: filas 4600000 a 4699999\n",
      "Chunk 4700000 guardado: filas 4700000 a 4799999\n",
      "Chunk 4800000 guardado: filas 4800000 a 4899999\n",
      "Chunk 4900000 guardado: filas 4900000 a 4999999\n",
      "Chunk 5000000 guardado: filas 5000000 a 5099999\n",
      "Chunk 5100000 guardado: filas 5100000 a 5199999\n",
      "Chunk 5200000 guardado: filas 5200000 a 5299999\n",
      "Chunk 5300000 guardado: filas 5300000 a 5399999\n",
      "Chunk 5400000 guardado: filas 5400000 a 5499999\n",
      "Chunk 5500000 guardado: filas 5500000 a 5599999\n",
      "Chunk 5600000 guardado: filas 5600000 a 5699999\n",
      "Chunk 5700000 guardado: filas 5700000 a 5799999\n",
      "Chunk 5800000 guardado: filas 5800000 a 5899999\n",
      "Chunk 5900000 guardado: filas 5900000 a 5999999\n",
      "Chunk 6000000 guardado: filas 6000000 a 6099999\n",
      "Chunk 6100000 guardado: filas 6100000 a 6199999\n",
      "Chunk 6200000 guardado: filas 6200000 a 6299999\n",
      "Chunk 6300000 guardado: filas 6300000 a 6399999\n",
      "Chunk 6400000 guardado: filas 6400000 a 6499999\n",
      "Chunk 6500000 guardado: filas 6500000 a 6599999\n",
      "Chunk 6600000 guardado: filas 6600000 a 6699999\n",
      "Chunk 6700000 guardado: filas 6700000 a 6771537\n",
      "CIC-IDS2017 limpiado guardado por chunks.\n"
     ]
    }
   ],
   "source": [
    "guardar_por_chunks(data_balanced, '../../data/processed/cic_ids2017_clean.csv')\n",
    "print(\"CIC-IDS2017 limpiado guardado por chunks.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
