{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c483d56-daad-445f-8e55-6dd2a354c66e",
   "metadata": {},
   "source": [
    "# 📊 Fase 2 – Dataset y Preprocesamiento\n",
    "\n",
    "## 🎯 Objetivo\n",
    "Preparar un conjunto de datos limpio y estructurado para entrenar un modelo de Machine Learning enfocado en la detección de intrusiones en redes, como parte del proyecto modular **CiberVigIA 2025B**.\n",
    "\n",
    "## 📁 Datasets sugeridos\n",
    "- **NSL-KDD**: Dataset clásico para sistemas de detección de intrusos (IDS), útil para benchmarking.\n",
    "- **CICIDS 2017**: Dataset más realista y contemporáneo, con tráfico simulado que incluye ataques modernos.\n",
    "\n",
    "## 🧪 Pasos técnicos\n",
    "\n",
    "1. **Descarga del dataset**  \n",
    "   - Formato: `.csv`  \n",
    "   - Fuente: Repositorio oficial o mirror institucional\n",
    "\n",
    "2. **Limpieza de datos**  \n",
    "   - Eliminación de columnas irrelevantes (por ejemplo: `timestamp`, `flow_id`, etc.)  \n",
    "   - Revisión de valores nulos o inconsistentes  \n",
    "   - Normalización de nombres de columnas\n",
    "\n",
    "3. **Codificación de variables categóricas**  \n",
    "   - Ejemplo: `protocol_type` → TCP / UDP / ICMP  \n",
    "   - Técnicas: One-Hot Encoding o Label Encoding según el modelo a utilizar\n",
    "\n",
    "4. **Verificación de balance de clases**  \n",
    "   - Identificación de clases mayoritarias/minoritarias  \n",
    "   - Posible aplicación de técnicas como SMOTE para balancear\n",
    "\n",
    "5. **Exportación del dataset preprocesado**  \n",
    "   - Formato final: `CSV` limpio y listo para entrenamiento  \n",
    "   - Ubicación: `data/processed/` dentro del repositorio\n",
    "\n",
    "## 📦 Entregable\n",
    "Un archivo `.csv` con los datos preprocesados, documentado y listo para ser utilizado en la Fase 3 (Entrenamiento del modelo ML).\n",
    "\n",
    "---\n",
    "\n",
    "> 🧠 *Este notebook forma parte del pipeline modular del proyecto CiberVigIA, alineado con objetivos de sostenibilidad, seguridad informática y análisis de tráfico de red mediante inteligencia artificial.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54120a46-612e-4f1a-a6fc-92f0d39d6538",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134e1bc1-0198-4d44-b290-7f373386e9ea",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Función de ayuda para obtener los nombres de las columnas en el dataset de KDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ca1a102-5b40-49c3-ab4f-c12f8b391539",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_column_names(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    column_names = []\n",
    "    attack_types_raw = lines[0].strip()  # Línea 0: tipos de ataques\n",
    "    attack_types = [attack.strip() for attack in attack_types_raw.split(',')]  # Parsear en lista\n",
    "    for line in lines:  # Empieza desde línea 2\n",
    "        if ':' in line:  # Ignora líneas sin \":\"\n",
    "            name = line.split(':')[0].strip()  # Toma antes de \":\", quita espacios\n",
    "            column_names.append(name)\n",
    "    column_names += ['label']\n",
    "    return attack_types, column_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98cb8d65-194d-402b-a96b-2a9046c68c17",
   "metadata": {},
   "source": [
    "### Paso 1. Carga de Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6e3319a-934c-414c-9b73-44e1cfa6ad02",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../data/raw/KDD CUP 99/corrected.gz', header=None)\n",
    "column_names = get_column_names('../../data/raw/KDD CUP 99/kddcup.names')\n",
    "attack_types, df.columns = column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3fb159be-0279-411d-b018-6b6a5330d569",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = pd.read_csv('../../data/raw/IDS 2017/Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv')\n",
    "data2 = pd.read_csv('../../data/raw/IDS 2017/Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv')\n",
    "data3 = pd.read_csv('../../data/raw/IDS 2017/Friday-WorkingHours-Morning.pcap_ISCX.csv')\n",
    "data4 = pd.read_csv('../../data/raw/IDS 2017/Monday-WorkingHours.pcap_ISCX.csv')\n",
    "data5 = pd.read_csv('../../data/raw/IDS 2017/Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv')\n",
    "data6 = pd.read_csv('../../data/raw/IDS 2017/Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv')\n",
    "data7 = pd.read_csv('../../data/raw/IDS 2017/Tuesday-WorkingHours.pcap_ISCX.csv')\n",
    "data8 = pd.read_csv('../../data/raw/IDS 2017/Wednesday-workingHours.pcap_ISCX.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8ae329-7aaf-4626-a381-93c10373602f",
   "metadata": {},
   "source": [
    "### Paso 2. Agrupar Dataset de CIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83a990b8-d5c4-4ecb-ae1b-c47e2cf649a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = [data1, data2, data3, data4, data5, data6, data7, data8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6e89d2b-fb83-45bb-a79d-370b19c41fda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data dimensions: \n",
      "Data1 -> 225745 rows, 79 columns\n",
      "Data2 -> 286467 rows, 79 columns\n",
      "Data3 -> 191033 rows, 79 columns\n",
      "Data4 -> 529918 rows, 79 columns\n",
      "Data5 -> 288602 rows, 79 columns\n",
      "Data6 -> 170366 rows, 79 columns\n",
      "Data7 -> 445909 rows, 79 columns\n",
      "Data8 -> 692703 rows, 79 columns\n"
     ]
    }
   ],
   "source": [
    "print('Data dimensions: ')\n",
    "for i, data in enumerate(data_list, start = 1):\n",
    "  rows, cols = data.shape\n",
    "  print(f'Data{i} -> {rows} rows, {cols} columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07a593ed-1e3d-4a97-8213-0b4f623329e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat(data_list) # Dataset CIC 2017 concatenado\n",
    "rows, cols = data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d3d42134-a482-4ba8-b261-1e6a11ab2086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New dimension for CIC Dataset:\n",
      "Number of rows: 2830743\n",
      "Number of columns: 79\n",
      "Total cells: 223628697\n"
     ]
    }
   ],
   "source": [
    "print('New dimension for CIC Dataset:')\n",
    "print(f'Number of rows: {rows}')\n",
    "print(f'Number of columns: {cols}')\n",
    "print(f'Total cells: {rows * cols}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ef13725-ffc8-4d48-9788-600a617fa86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in data_list: del d # Limpieza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1feaa7c0-f9a9-4005-953f-9c5c49f1734e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data KDD -> 311029 rows, 42 columns\n"
     ]
    }
   ],
   "source": [
    "rows, cols = df.shape\n",
    "print(f'Data KDD -> {rows} rows, {cols} columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067fcc61-3374-4fd5-8eba-7935ce8cb467",
   "metadata": {},
   "source": [
    "### Paso 3: Mostrar los nombres de las columnas y exploración"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "84d9888d-d00b-45b9-a441-a304712f79ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data KDD -> Index(['duration', 'protocol_type', 'service', 'flag', 'src_bytes',\n",
      "       'dst_bytes', 'land', 'wrong_fragment', 'urgent', 'hot',\n",
      "       'num_failed_logins', 'logged_in', 'num_compromised', 'root_shell',\n",
      "       'su_attempted', 'num_root', 'num_file_creations', 'num_shells',\n",
      "       'num_access_files', 'num_outbound_cmds', 'is_host_login',\n",
      "       'is_guest_login', 'count', 'srv_count', 'serror_rate',\n",
      "       'srv_serror_rate', 'rerror_rate', 'srv_rerror_rate', 'same_srv_rate',\n",
      "       'diff_srv_rate', 'srv_diff_host_rate', 'dst_host_count',\n",
      "       'dst_host_srv_count', 'dst_host_same_srv_rate',\n",
      "       'dst_host_diff_srv_rate', 'dst_host_same_src_port_rate',\n",
      "       'dst_host_srv_diff_host_rate', 'dst_host_serror_rate',\n",
      "       'dst_host_srv_serror_rate', 'dst_host_rerror_rate',\n",
      "       'dst_host_srv_rerror_rate', 'label'],\n",
      "      dtype='object')\n",
      "Data CIC -> Index([' Destination Port', ' Flow Duration', ' Total Fwd Packets',\n",
      "       ' Total Backward Packets', 'Total Length of Fwd Packets',\n",
      "       ' Total Length of Bwd Packets', ' Fwd Packet Length Max',\n",
      "       ' Fwd Packet Length Min', ' Fwd Packet Length Mean',\n",
      "       ' Fwd Packet Length Std', 'Bwd Packet Length Max',\n",
      "       ' Bwd Packet Length Min', ' Bwd Packet Length Mean',\n",
      "       ' Bwd Packet Length Std', 'Flow Bytes/s', ' Flow Packets/s',\n",
      "       ' Flow IAT Mean', ' Flow IAT Std', ' Flow IAT Max', ' Flow IAT Min',\n",
      "       'Fwd IAT Total', ' Fwd IAT Mean', ' Fwd IAT Std', ' Fwd IAT Max',\n",
      "       ' Fwd IAT Min', 'Bwd IAT Total', ' Bwd IAT Mean', ' Bwd IAT Std',\n",
      "       ' Bwd IAT Max', ' Bwd IAT Min', 'Fwd PSH Flags', ' Bwd PSH Flags',\n",
      "       ' Fwd URG Flags', ' Bwd URG Flags', ' Fwd Header Length',\n",
      "       ' Bwd Header Length', 'Fwd Packets/s', ' Bwd Packets/s',\n",
      "       ' Min Packet Length', ' Max Packet Length', ' Packet Length Mean',\n",
      "       ' Packet Length Std', ' Packet Length Variance', 'FIN Flag Count',\n",
      "       ' SYN Flag Count', ' RST Flag Count', ' PSH Flag Count',\n",
      "       ' ACK Flag Count', ' URG Flag Count', ' CWE Flag Count',\n",
      "       ' ECE Flag Count', ' Down/Up Ratio', ' Average Packet Size',\n",
      "       ' Avg Fwd Segment Size', ' Avg Bwd Segment Size',\n",
      "       ' Fwd Header Length.1', 'Fwd Avg Bytes/Bulk', ' Fwd Avg Packets/Bulk',\n",
      "       ' Fwd Avg Bulk Rate', ' Bwd Avg Bytes/Bulk', ' Bwd Avg Packets/Bulk',\n",
      "       'Bwd Avg Bulk Rate', 'Subflow Fwd Packets', ' Subflow Fwd Bytes',\n",
      "       ' Subflow Bwd Packets', ' Subflow Bwd Bytes', 'Init_Win_bytes_forward',\n",
      "       ' Init_Win_bytes_backward', ' act_data_pkt_fwd',\n",
      "       ' min_seg_size_forward', 'Active Mean', ' Active Std', ' Active Max',\n",
      "       ' Active Min', 'Idle Mean', ' Idle Std', ' Idle Max', ' Idle Min',\n",
      "       ' Label'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(f'Data KDD -> {df.columns}')\n",
    "print(f'Data CIC -> {data.columns}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "96fe8c70-6db4-4f79-837c-39adf6bacf79",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns = data.columns.str.lstrip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3899fb21-5dc2-41fb-ac58-3fcbb0622abc",
   "metadata": {},
   "source": [
    "Limpiamos las columnas no necesarias de cada uno:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3448f169-cb3c-44a7-9b1a-3c15ec79dbcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No capturables quitadas. Nueva forma: (311029, 30)\n"
     ]
    }
   ],
   "source": [
    "non_capturable = [\n",
    "    'hot', # Host-Based\n",
    "    'num_failed_logins', # Requiere logs host.\n",
    "    'logged_in', # Host-Based\n",
    "    'num_compromised', # Correlación externa necesatia \n",
    "    'root_shell', # No es fiable\n",
    "    'su_attempt', # Depende de payload\n",
    "    'num_root', # Requiere logs host.\n",
    "    'num_file_creations',  # Requiere host monitoring o inspección profunda.\n",
    "    'num_shells', # No fiable en encriptado\n",
    "    'num_access_files', # Host-Based\n",
    "    'num_outbound_cmds', # Parsing profundo\n",
    "    'is_host_login', # No siempre capturable\n",
    "    'is_guest_login', # No siempre capturable\n",
    "]\n",
    "\n",
    "df = df.drop(columns=[col for col in non_capturable if col in df.columns])\n",
    "print(\"No capturables quitadas. Nueva forma:\", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "25ad3081-13b4-404a-bbc5-b9d458d00361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data KDD:\n",
      "Forma original: (311029, 30)\n",
      "Tipos: duration                         int64\n",
      "protocol_type                   object\n",
      "service                         object\n",
      "flag                            object\n",
      "src_bytes                        int64\n",
      "dst_bytes                        int64\n",
      "land                             int64\n",
      "wrong_fragment                   int64\n",
      "urgent                           int64\n",
      "su_attempted                     int64\n",
      "count                            int64\n",
      "srv_count                        int64\n",
      "serror_rate                    float64\n",
      "srv_serror_rate                float64\n",
      "rerror_rate                    float64\n",
      "srv_rerror_rate                float64\n",
      "same_srv_rate                  float64\n",
      "diff_srv_rate                  float64\n",
      "srv_diff_host_rate             float64\n",
      "dst_host_count                   int64\n",
      "dst_host_srv_count               int64\n",
      "dst_host_same_srv_rate         float64\n",
      "dst_host_diff_srv_rate         float64\n",
      "dst_host_same_src_port_rate    float64\n",
      "dst_host_srv_diff_host_rate    float64\n",
      "dst_host_serror_rate           float64\n",
      "dst_host_srv_serror_rate       float64\n",
      "dst_host_rerror_rate           float64\n",
      "dst_host_srv_rerror_rate       float64\n",
      "label                           object\n",
      "dtype: object\n",
      "Missing values: 0\n",
      "Duplicados: 233766\n",
      "Balance de labels: label\n",
      "smurf.              164091\n",
      "normal.              60593\n",
      "neptune.             58001\n",
      "snmpgetattack.        7741\n",
      "mailbomb.             5000\n",
      "guess_passwd.         4367\n",
      "snmpguess.            2406\n",
      "satan.                1633\n",
      "warezmaster.          1602\n",
      "back.                 1098\n",
      "mscan.                1053\n",
      "apache2.               794\n",
      "processtable.          759\n",
      "saint.                 736\n",
      "portsweep.             354\n",
      "ipsweep.               306\n",
      "httptunnel.            158\n",
      "pod.                    87\n",
      "nmap.                   84\n",
      "buffer_overflow.        22\n",
      "multihop.               18\n",
      "named.                  17\n",
      "sendmail.               17\n",
      "ps.                     16\n",
      "rootkit.                13\n",
      "xterm.                  13\n",
      "teardrop.               12\n",
      "xlock.                   9\n",
      "land.                    9\n",
      "xsnoop.                  4\n",
      "ftp_write.               3\n",
      "loadmodule.              2\n",
      "perl.                    2\n",
      "udpstorm.                2\n",
      "worm.                    2\n",
      "phf.                     2\n",
      "sqlattack.               2\n",
      "imap.                    1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print('Data KDD:')\n",
    "print(\"Forma original:\", df.shape)\n",
    "print(\"Tipos:\", df.dtypes)\n",
    "print(\"Missing values:\", df.isnull().sum().sum())\n",
    "print(\"Duplicados:\", df.duplicated().sum())\n",
    "label_counts = df['label'].value_counts() if 'label' in df else pd.Series()\n",
    "print(\"Balance de labels:\", label_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36866ac5-a04d-4fdd-8142-de96cc4598bd",
   "metadata": {},
   "source": [
    "Filtrar clases con pocas muestras (evita error SMOTE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "54960358-de45-4bf5-b36f-dd34e5e91b4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtrando clases con <2 muestras: Index(['imap.'], dtype='object', name='label')\n"
     ]
    }
   ],
   "source": [
    "low_sample_classes = label_counts[label_counts < 2].index\n",
    "if len(low_sample_classes) > 0:\n",
    "    print(f\"Filtrando clases con <2 muestras: {low_sample_classes}\")\n",
    "    df = df[~df['label'].isin(low_sample_classes)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4cd3481b-f7a9-44fb-8090-8bf4419c209a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data CIC:\n",
      "Forma original: (2830743, 79)\n",
      "Tipos: Destination Port                 int64\n",
      "Flow Duration                    int64\n",
      "Total Fwd Packets                int64\n",
      "Total Backward Packets           int64\n",
      "Total Length of Fwd Packets      int64\n",
      "                                ...   \n",
      "Idle Mean                      float64\n",
      "Idle Std                       float64\n",
      "Idle Max                         int64\n",
      "Idle Min                         int64\n",
      "Label                           object\n",
      "Length: 79, dtype: object\n",
      "Missing values: 1358\n",
      "Duplicados: 308381\n",
      "Balance de labels: Label\n",
      "BENIGN                        2273097\n",
      "DoS Hulk                       231073\n",
      "PortScan                       158930\n",
      "DDoS                           128027\n",
      "DoS GoldenEye                   10293\n",
      "FTP-Patator                      7938\n",
      "SSH-Patator                      5897\n",
      "DoS slowloris                    5796\n",
      "DoS Slowhttptest                 5499\n",
      "Bot                              1966\n",
      "Web Attack � Brute Force         1507\n",
      "Web Attack � XSS                  652\n",
      "Infiltration                       36\n",
      "Web Attack � Sql Injection         21\n",
      "Heartbleed                         11\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print('Data CIC:')\n",
    "print(\"Forma original:\", data.shape)\n",
    "print(\"Tipos:\", data.dtypes)\n",
    "print(\"Missing values:\", data.isnull().sum().sum())\n",
    "print(\"Duplicados:\", data.duplicated().sum())\n",
    "label_counts = data['Label'].value_counts() if 'Label' in data else pd.Series()\n",
    "print(\"Balance de labels:\", label_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a548701-fc89-46bd-8d9c-243af898813c",
   "metadata": {},
   "source": [
    "Filtrar clases con pocas muestras (evita error SMOTE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8cb44020-0e82-4a51-9b34-0156384d49e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "low_sample_classes = label_counts[label_counts < 2].index\n",
    "if len(low_sample_classes) > 0:\n",
    "    print(f\"Filtrando clases con <2 muestras: {low_sample_classes}\")\n",
    "    data = data[~data['Label'].isin(low_sample_classes)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa16a305-7c10-412b-9b41-aaa7b70d50ef",
   "metadata": {},
   "source": [
    "### Paso 4: Limpiamos los datos perdidos / duplicados / infinitos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "44546a8e-2c14-4c7b-b03b-175d0f941a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.replace([np.inf, -np.inf], np.nan).fillna(0).drop_duplicates()\n",
    "data = data.replace([np.inf, -np.inf], np.nan).fillna(0).drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d24ff16-d6f2-416e-bb0c-cdca0017aa76",
   "metadata": {},
   "source": [
    "### Paso 5: Agegamos encodings categoricos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f722bc06-fa9f-4e66-9a9b-7239fbf7831f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lekdd = LabelEncoder()\n",
    "for col in ['protocol_type', 'service', 'flag']:\n",
    "    df[col] = lekdd.fit_transform(df[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f644fd17-bccf-4ce9-8a4f-b5d0478cea2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lecic = LabelEncoder()\n",
    "for col in data.select_dtypes(include=['object']).columns:\n",
    "    if col != 'Label':\n",
    "        data[col] = lecic.fit_transform(data[col])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e70143-5a5a-4072-8fe0-bddd42ef4452",
   "metadata": {},
   "source": [
    "### Paso 6: Scaling numerico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0251b515-9525-40d8-ae73-9e3ca6e76bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_cols_kdd = df.select_dtypes(include=['int64', 'float64']).columns.drop('label', errors='ignore')\n",
    "scaler_kdd = StandardScaler()\n",
    "df[numerical_cols_kdd] = scaler_kdd.fit_transform(df[numerical_cols_kdd])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "920735a4-c63a-48ac-bb90-c3b94ff07cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_cols_cic = data.select_dtypes(include=['int64', 'float64']).columns.drop('Label', errors='ignore')\n",
    "scaler_cic = StandardScaler()\n",
    "data[numerical_cols_cic] = scaler_cic.fit_transform(data[numerical_cols_cic])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e742269b-0d9a-483d-b5c0-8373d28a5c06",
   "metadata": {},
   "source": [
    "### Paso 7: Balanceo (SMOTE) -> Se necesita en ambos datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "10be0194-6fb7-4a15-a681-d59c7f253c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smote_por_chunks(X, y, chunk_size=50000, random_state=42, k_neighbors=1):\n",
    "    \"\"\"\n",
    "    Aplica SMOTE por bloques. Si SMOTE falla en un chunk, usa RandomOverSampler como respaldo.\n",
    "    Retorna un DataFrame balanceado.\n",
    "    \"\"\"\n",
    "    X_res_total, y_res_total = [], []\n",
    "\n",
    "    # Identifica la clase minoritaria\n",
    "    clase_min = y.value_counts().idxmin()\n",
    "\n",
    "    # Separa las clases\n",
    "    X_min, y_min = X[y == clase_min], y[y == clase_min]\n",
    "    X_maj, y_maj = X[y != clase_min], y[y != clase_min]\n",
    "\n",
    "    # Itera sobre la clase mayoritaria en bloques\n",
    "    for i in range(0, len(X_maj), chunk_size):\n",
    "        X_chunk = pd.concat([X_min, X_maj.iloc[i:i + chunk_size]])\n",
    "        y_chunk = pd.concat([y_min, y_maj.iloc[i:i + chunk_size]])\n",
    "\n",
    "        try:\n",
    "            # Intenta aplicar SMOTE\n",
    "            smote = SMOTE(random_state=random_state, k_neighbors=k_neighbors)\n",
    "            X_res, y_res = smote.fit_resample(X_chunk, y_chunk)\n",
    "            print(f\"Chunk {i}: SMOTE aplicado correctamente.\")\n",
    "        except Exception as e:\n",
    "            # Si SMOTE falla, usa RandomOverSampler\n",
    "            print(f\"Chunk {i} falló con SMOTE: {e}. Usando RandomOverSampler.\")\n",
    "            ros = RandomOverSampler(random_state=random_state)\n",
    "            X_res, y_res = ros.fit_resample(X_chunk, y_chunk)\n",
    "\n",
    "        # Guarda los resultados del chunk\n",
    "        X_res_total.append(pd.DataFrame(X_res, columns=X.columns))\n",
    "        y_res_total.append(pd.Series(y_res))\n",
    "\n",
    "    # Une todos los chunks procesados\n",
    "    X_final = pd.concat(X_res_total, ignore_index=True)\n",
    "    y_final = pd.concat(y_res_total, ignore_index=True)\n",
    "\n",
    "    return pd.concat([X_final, pd.Series(y_final, name='Label')], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d7066f83-fa48-4697-a14c-a09e95d493ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 0 falló con SMOTE: Expected n_neighbors <= n_samples_fit, but n_neighbors = 2, n_samples_fit = 1, n_samples = 1. Usando RandomOverSampler.\n",
      "Chunk 50000 falló con SMOTE: Expected n_neighbors <= n_samples_fit, but n_neighbors = 2, n_samples_fit = 1, n_samples = 1. Usando RandomOverSampler.\n"
     ]
    }
   ],
   "source": [
    "X = df.drop('label', axis=1)\n",
    "y = df['label']\n",
    "df_balanced = smote_por_chunks(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fed2a163-da02-4e49-a706-4c0f6a840dc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 0: SMOTE aplicado correctamente.\n",
      "Chunk 50000: SMOTE aplicado correctamente.\n",
      "Chunk 100000: SMOTE aplicado correctamente.\n",
      "Chunk 150000: SMOTE aplicado correctamente.\n",
      "Chunk 200000: SMOTE aplicado correctamente.\n",
      "Chunk 250000: SMOTE aplicado correctamente.\n",
      "Chunk 300000: SMOTE aplicado correctamente.\n",
      "Chunk 350000: SMOTE aplicado correctamente.\n",
      "Chunk 400000: SMOTE aplicado correctamente.\n",
      "Chunk 450000: SMOTE aplicado correctamente.\n",
      "Chunk 500000: SMOTE aplicado correctamente.\n",
      "Chunk 550000: SMOTE aplicado correctamente.\n",
      "Chunk 600000: SMOTE aplicado correctamente.\n",
      "Chunk 650000: SMOTE aplicado correctamente.\n",
      "Chunk 700000: SMOTE aplicado correctamente.\n",
      "Chunk 750000: SMOTE aplicado correctamente.\n",
      "Chunk 800000: SMOTE aplicado correctamente.\n",
      "Chunk 850000: SMOTE aplicado correctamente.\n",
      "Chunk 900000: SMOTE aplicado correctamente.\n",
      "Chunk 950000: SMOTE aplicado correctamente.\n",
      "Chunk 1000000: SMOTE aplicado correctamente.\n",
      "Chunk 1050000: SMOTE aplicado correctamente.\n",
      "Chunk 1100000: SMOTE aplicado correctamente.\n",
      "Chunk 1150000: SMOTE aplicado correctamente.\n",
      "Chunk 1200000: SMOTE aplicado correctamente.\n",
      "Chunk 1250000: SMOTE aplicado correctamente.\n",
      "Chunk 1300000: SMOTE aplicado correctamente.\n",
      "Chunk 1350000: SMOTE aplicado correctamente.\n",
      "Chunk 1400000: SMOTE aplicado correctamente.\n",
      "Chunk 1450000: SMOTE aplicado correctamente.\n",
      "Chunk 1500000: SMOTE aplicado correctamente.\n",
      "Chunk 1550000: SMOTE aplicado correctamente.\n",
      "Chunk 1600000: SMOTE aplicado correctamente.\n",
      "Chunk 1650000: SMOTE aplicado correctamente.\n",
      "Chunk 1700000: SMOTE aplicado correctamente.\n",
      "Chunk 1750000: SMOTE aplicado correctamente.\n",
      "Chunk 1800000: SMOTE aplicado correctamente.\n",
      "Chunk 1850000: SMOTE aplicado correctamente.\n",
      "Chunk 1900000 falló con SMOTE: Expected n_neighbors <= n_samples_fit, but n_neighbors = 2, n_samples_fit = 1, n_samples = 1. Usando RandomOverSampler.\n",
      "Chunk 1950000: SMOTE aplicado correctamente.\n",
      "Chunk 2000000: SMOTE aplicado correctamente.\n",
      "Chunk 2050000: SMOTE aplicado correctamente.\n",
      "Chunk 2100000: SMOTE aplicado correctamente.\n",
      "Chunk 2150000: SMOTE aplicado correctamente.\n",
      "Chunk 2200000: SMOTE aplicado correctamente.\n",
      "Chunk 2250000: SMOTE aplicado correctamente.\n",
      "Chunk 2300000: SMOTE aplicado correctamente.\n",
      "Chunk 2350000: SMOTE aplicado correctamente.\n",
      "Chunk 2400000: SMOTE aplicado correctamente.\n",
      "Chunk 2450000: SMOTE aplicado correctamente.\n",
      "Chunk 2500000: SMOTE aplicado correctamente.\n"
     ]
    }
   ],
   "source": [
    "X = data.drop('Label', axis=1)\n",
    "y = data['Label']\n",
    "data_balanced = smote_por_chunks(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd809473-8c13-4d47-86a0-a25579cb713e",
   "metadata": {},
   "source": [
    "### Paso 8: Guardar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4bf166e8-2a2a-4505-a7f3-461305509e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def guardar_por_chunks(df, ruta_csv, chunk_size=100000):\n",
    "    # Si el archivo ya existe, lo eliminamos para empezar limpio\n",
    "    import os\n",
    "    if os.path.exists(ruta_csv):\n",
    "        os.remove(ruta_csv)\n",
    "\n",
    "    # Guardar por bloques\n",
    "    for i in range(0, len(df), chunk_size):\n",
    "        chunk = df.iloc[i:i+chunk_size]\n",
    "        # Solo escribe el encabezado en el primer chunk\n",
    "        chunk.to_csv(ruta_csv, mode='a', index=False, header=(i==0))\n",
    "        print(f\"Chunk {i} guardado: filas {i} a {i+len(chunk)-1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cf78286f-3bc0-4056-88fc-ebdb3f7f122d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 0 guardado: filas 0 a 99999\n",
      "Chunk 100000 guardado: filas 100000 a 199999\n",
      "Chunk 200000 guardado: filas 200000 a 299999\n",
      "Chunk 300000 guardado: filas 300000 a 399999\n",
      "Chunk 400000 guardado: filas 400000 a 499999\n",
      "Chunk 500000 guardado: filas 500000 a 599999\n",
      "Chunk 600000 guardado: filas 600000 a 699999\n",
      "Chunk 700000 guardado: filas 700000 a 799999\n",
      "Chunk 800000 guardado: filas 800000 a 899999\n",
      "Chunk 900000 guardado: filas 900000 a 999999\n",
      "Chunk 1000000 guardado: filas 1000000 a 1099999\n",
      "Chunk 1100000 guardado: filas 1100000 a 1199999\n",
      "Chunk 1200000 guardado: filas 1200000 a 1299999\n",
      "Chunk 1300000 guardado: filas 1300000 a 1399999\n",
      "Chunk 1400000 guardado: filas 1400000 a 1450115\n",
      "NSL-KDD limpiado guardado por chunks.\n"
     ]
    }
   ],
   "source": [
    "guardar_por_chunks(df_balanced, '../../data/processed/nsl_kdd_clean.csv')\n",
    "print(\"NSL-KDD limpiado guardado por chunks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f36aeb76-a5ef-49af-86c5-ba9e0bfc3b69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 0 guardado: filas 0 a 99999\n",
      "Chunk 100000 guardado: filas 100000 a 199999\n",
      "Chunk 200000 guardado: filas 200000 a 299999\n",
      "Chunk 300000 guardado: filas 300000 a 399999\n",
      "Chunk 400000 guardado: filas 400000 a 499999\n",
      "Chunk 500000 guardado: filas 500000 a 599999\n",
      "Chunk 600000 guardado: filas 600000 a 699999\n",
      "Chunk 700000 guardado: filas 700000 a 799999\n",
      "Chunk 800000 guardado: filas 800000 a 899999\n",
      "Chunk 900000 guardado: filas 900000 a 999999\n",
      "Chunk 1000000 guardado: filas 1000000 a 1099999\n",
      "Chunk 1100000 guardado: filas 1100000 a 1199999\n",
      "Chunk 1200000 guardado: filas 1200000 a 1299999\n",
      "Chunk 1300000 guardado: filas 1300000 a 1399999\n",
      "Chunk 1400000 guardado: filas 1400000 a 1499999\n",
      "Chunk 1500000 guardado: filas 1500000 a 1599999\n",
      "Chunk 1600000 guardado: filas 1600000 a 1699999\n",
      "Chunk 1700000 guardado: filas 1700000 a 1799999\n",
      "Chunk 1800000 guardado: filas 1800000 a 1899999\n",
      "Chunk 1900000 guardado: filas 1900000 a 1999999\n",
      "Chunk 2000000 guardado: filas 2000000 a 2099999\n",
      "Chunk 2100000 guardado: filas 2100000 a 2199999\n",
      "Chunk 2200000 guardado: filas 2200000 a 2299999\n",
      "Chunk 2300000 guardado: filas 2300000 a 2399999\n",
      "Chunk 2400000 guardado: filas 2400000 a 2499999\n",
      "Chunk 2500000 guardado: filas 2500000 a 2599999\n",
      "Chunk 2600000 guardado: filas 2600000 a 2699999\n",
      "Chunk 2700000 guardado: filas 2700000 a 2799999\n",
      "Chunk 2800000 guardado: filas 2800000 a 2899999\n",
      "Chunk 2900000 guardado: filas 2900000 a 2999999\n",
      "Chunk 3000000 guardado: filas 3000000 a 3099999\n",
      "Chunk 3100000 guardado: filas 3100000 a 3199999\n",
      "Chunk 3200000 guardado: filas 3200000 a 3299999\n",
      "Chunk 3300000 guardado: filas 3300000 a 3399999\n",
      "Chunk 3400000 guardado: filas 3400000 a 3499999\n",
      "Chunk 3500000 guardado: filas 3500000 a 3599999\n",
      "Chunk 3600000 guardado: filas 3600000 a 3699999\n",
      "Chunk 3700000 guardado: filas 3700000 a 3799999\n",
      "Chunk 3800000 guardado: filas 3800000 a 3899999\n",
      "Chunk 3900000 guardado: filas 3900000 a 3999999\n",
      "Chunk 4000000 guardado: filas 4000000 a 4099999\n",
      "Chunk 4100000 guardado: filas 4100000 a 4199999\n",
      "Chunk 4200000 guardado: filas 4200000 a 4299999\n",
      "Chunk 4300000 guardado: filas 4300000 a 4399999\n",
      "Chunk 4400000 guardado: filas 4400000 a 4499999\n",
      "Chunk 4500000 guardado: filas 4500000 a 4599999\n",
      "Chunk 4600000 guardado: filas 4600000 a 4699999\n",
      "Chunk 4700000 guardado: filas 4700000 a 4799999\n",
      "Chunk 4800000 guardado: filas 4800000 a 4899999\n",
      "Chunk 4900000 guardado: filas 4900000 a 4999999\n",
      "Chunk 5000000 guardado: filas 5000000 a 5099999\n",
      "Chunk 5100000 guardado: filas 5100000 a 5199999\n",
      "Chunk 5200000 guardado: filas 5200000 a 5299999\n",
      "Chunk 5300000 guardado: filas 5300000 a 5399999\n",
      "Chunk 5400000 guardado: filas 5400000 a 5499999\n",
      "Chunk 5500000 guardado: filas 5500000 a 5599999\n",
      "Chunk 5600000 guardado: filas 5600000 a 5699999\n",
      "Chunk 5700000 guardado: filas 5700000 a 5799999\n",
      "Chunk 5800000 guardado: filas 5800000 a 5899999\n",
      "Chunk 5900000 guardado: filas 5900000 a 5999999\n",
      "Chunk 6000000 guardado: filas 6000000 a 6099999\n",
      "Chunk 6100000 guardado: filas 6100000 a 6199999\n",
      "Chunk 6200000 guardado: filas 6200000 a 6299999\n",
      "Chunk 6300000 guardado: filas 6300000 a 6399999\n",
      "Chunk 6400000 guardado: filas 6400000 a 6499999\n",
      "Chunk 6500000 guardado: filas 6500000 a 6599999\n",
      "Chunk 6600000 guardado: filas 6600000 a 6699999\n",
      "Chunk 6700000 guardado: filas 6700000 a 6771537\n",
      "CIC-IDS2017 limpiado guardado por chunks.\n"
     ]
    }
   ],
   "source": [
    "guardar_por_chunks(data_balanced, '../../data/processed/cic_ids2017_clean.csv')\n",
    "print(\"CIC-IDS2017 limpiado guardado por chunks.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
